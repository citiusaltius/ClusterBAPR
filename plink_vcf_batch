#!/bin/tcsh
#
# Example shell script for jobs on the Center for Simulation and Modeling (SaM) cluster (Frank)
# $Revision: 1.0 $
# $Date: 03/07/13 01:37:01PM $
# $Author: barmada $
#
# Note that the switches given for the shell can be modified to give more output from the script. In particular, -vx will output each line of the script as it is read, and then repeat the line with any variable substitutions
#       This can be pretty handy if you have a script that is dying and you don't know where the problem is occurring
#
# PBS control switches:
#   all PBS control switches can be inserted inside the scripts, preceeded by pound-PBS (#PBS)
#   some common switches:
#     -a date_time         run the script at a prespecified date/time
#     -l resource_list     request the specified resources - as an example, -l nodes=1:ppn=1 requests one node, and one processor (core) on that node for the job. Some common resource requests:
#     -l nodes=1:ppn=1     nodes = number of nodes (physical compute nodes) requested; ppn = processes-per-node = number of cores requested (default: nodes=1:ppn=1)
#     -l mem=8GB           request a memory allocation of 8Gb for the job
#     -l file=500GB        request a node with at least 500GB of free scratch space
#     -l walltime=24:00:00 request an allowable (maximum) runtime for your job (default: 10 min)
#     -j oe                merge stdout and stderr stream of job into one output file per job
#     -q queue_name        specify the queue in which the job should be run (required - no default, though most users will use -q shared to access the general purpose shared queue)
#     -N name              specify job name
#     -S shell             command interpreter to be used (required if you want to use other than bash shell)
#     -v variable_list     export these environment variables
#     -@ file              read commandline input from file
#     -t array_request     create a job array (executing a specified number of replicates of your script), where the index number of a particular replicate is given by the $PBS_ARRAYID environment variable
#
# PBS environment variables
#       all PBS jobs inherit certain environment variables which can be referenced in the scripts:
#               $PBS_O_HOME             your home directory (on the node you execute your qsub command)
#               $PBS_O_WORKDIR  the directory from which you executed (qsub) your job
#               $SCRATCH                a temporary space on the local drive of the compute node your job executes on - to be used for local staging of job files
#               $PBS_ARRAYID    for job arrays, the index of the specific job replicate
echo '====================================' 
echo 'PLINK VCF batch converter' 
#PBS -o $PBS_O_HOME/ClusterBAPR/results
#PBS -e $PBS_O_HOME/ClusterBAPR/results
#PBS -j oe
#PBS -N index_refs
#PBS -q shared
#PBS -l nodes=1:ppn=1
#PBS -l walltime=0:01:00
#PBS -l mem=5GB
#PBS -l file=500GB
#PBS -S /bin/tcsh

#####
# This will tell you which host your job ran on:
echo JOB_ID: $PBS_JOBID JOB_NAME: $PBS_JOBNAME HOSTNAME: $PBS_O_HOST
echo start_time: `date`

##### 
# your prep code (module loading, variable initialization, file prep, etc.) goes here - things that don't require a lot of input/output
# module load genomics
# module load R

#####
echo PBS_O_HOME: $PBS_O_HOME
echo PBS_O_WORKDIR: $PBS_O_WORKDIR
# echo PBS_ARRAYID: $PBS_ARRAYID

#####
# code to stage require files to local scratch space:
echo staging_time: `date`
echo "SCRATCH directory listing\n\n" > scratch_directory_listing

# cp -R $PBS_O_WORKDIR/necessary_files $SCRATCH   # copy necessary files to tmp space - sometimes it might be a good idea to have your input files (if they're big) compressed beforehand, so you only have to copy compressed files

# copy ClusterBAPR files
mkdir $SCRATCH/ClusterBAPR
cp -R $PBS_O_HOME/ClusterBAPR/* $SCRATCH/ClusterBAPR
echo "--------------------\n" >> scratch_directory_listing
echo "$SCRATCH/ClusterBAPR" >> scratch_directory_listing
ls -l $SCRATCH/ClusterBAPR >> scratch_directory_listing

# copy PLINK 1.9
mkdir $SCRATCH/plink
cp $PBS_O_HOME/tools/plink-1.9/* $SCRATCH/plink
echo "--------------------\n" >> scratch_directory_listing
echo "$SCRATCH/plink" >> scratch_directory_listing
ls -l $SCRATCH/plink >> scratch_directory_listing

# copy Kamboh data over
# mkdir $SCRATCH/Kamboh
# cp $PBS_O_HOME/Kamboh/* $SCRATCH/Kamboh


echo staged_time: `date`

mv scratch_directory_listing $SCRATCH
cd $SCRATCH                                     # move to local scratch space. If you copied over compressed files, you might have a line here to uncompress those files as well

# verify a list of files transferred
echo "--------------------\n" >> scratch_directory_listing
echo "$SCRATCH" >> scratch_directory_listing
ls -l >> scratch_directory_listing

# create results folder
mkdir results
mv scratch_directory_listing results/

# results_files

#####
# the input/output or computationally intensive part of your script goes here

./plink/plink --ped ./plink/toy.ped --map ./plink/toy.map --make-bed --out ./results/toy-bed

 
#####
# code to return results files to your working directory
mkdir $PBS_O_WORKDIR/$PBS_JOBID
cp -r ./results  $PBS_O_WORKDIR/results/$PBS_JOBID # don't forget to copy those results files that are important back to your working directory!
cp scratch_directory_listing $PBS_O_WORKDIR/results/scratch_directory_listing

#####
# other clean up items, loop increments, etc., can go here


echo end_time: `date`
exit
